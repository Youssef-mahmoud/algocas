{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup:  \n",
    "- conda env remove -n qwen-local-ocr  \n",
    "  \n",
    "- conda create -n qwen-local-ocr -c conda-forge -y  \n",
    "- conda activate qwen-local-ocr   \n",
    "- conda install python requests huggingface_hub \"accelerate>=0.26.0\" pillow ipykernel jupyter nb_conda_kernels ipywidgets -c conda-forge -y  \n",
    "\n",
    "- pip install git+https://github.com/huggingface/transformers  \n",
    "- pip install qwen-vl-utils  \n",
    "  \n",
    "#### Install pytorch locally:  \n",
    "- https://pytorch.org/get-started/locally/  \n",
    "- pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  \n",
    "  \n",
    "python -m ipykernel install [--user] --prefix=C:\\Users\\tech_expert\\.conda\\envs\\qwen-local-ocr --name qwen-local-ocr  \n",
    "  \n",
    "Run VSCode in Admin mode to enable symbolic links created by model cache  \n",
    "  \n",
    "#### Model cache:  \n",
    "- C:\\Users\\ [username] \\.cache\\huggingface\\hub\\models--Qwen--Qwen2*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GetModelList import get_qwen_models \n",
    "import huggingface_hub\n",
    "from my_timer import MyTimer, my_timer\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_qwen_models(\"Qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device_map selects the processor to use, cpu or cuda (gpu)  \n",
    "usually select auto if you are going to run this code on different machines and you are not sure if they will have a GPU available.\n",
    "\n",
    "**This command creates an instance of the Qwen2_5_VLForConditionalGeneration class from the transformers library to load a pre-trained Qwen-2.5-VL model  from the Hugging Face model hub for image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    # \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "    # \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"cpu\"\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/huggingface/transformers.git@1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this command loads the appropriate processor for a pre-trained model from the Hugging Face transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = r\"..\\images\\dl1.jpg\"\n",
    "# prompt = \"Extract all text found on the image, including handwritten signatures\"\n",
    "\n",
    "image = r\"..\\images\\WalmartReceipt.png\"\n",
    "prompt = \"Extract all text found on the image, including handwritten signatures\"\n",
    "\n",
    "# image = r\"..\\images\\WalmartReceipt.png\"\n",
    "# prompt = \"What is the account number shown on this image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"image\": image,\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": prompt},\n",
    "    ],\n",
    "}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This statement uses the apply_chat_template method of the processor to format a list of messages into the specific input format expected by the  model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prepare the image(s) or video for the model, transforming them into a format the model can understand. This may include normalization, resizing and other transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the processor to prepare the input data (text, images, and potentially videos) for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move the data or tensors contained within the inputs dictionary to the GPU (Graphics Processing Unit) memory. Cuda is used for NVidia GPU's\n",
    "*** test the model.to method!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# new!!\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Inference, generate output from the model using our message and prompt - slowest part of the code\n",
    "max_new_tokens controls the model output, too large of a value can cause loops of output, where output is repeated several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create an iterator of tuples between input and generated ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decode model output into human readable format\n",
    "generated_ids_trimmed is the model output, which is a list of pytorch tensors containing tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-local-ocr",
   "language": "python",
   "name": "qwen-local-ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
