{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community pymupdf beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b103860",
   "metadata": {},
   "source": [
    "### Loading Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31 documents total.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, WebBaseLoader\n",
    "\n",
    "# --- PDF Loader ---\n",
    "pdf_path = \"PDFs/Erickson_Kretschmer_Mendis_chapter_4_PD.pdf\"  \n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# --- Web Page Loader ---\n",
    "web_url = \"https://medium.com/@vikrampande783/introduction-to-langchain-9e09aae37e62\"\n",
    "web_loader = WebBaseLoader(web_url)\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# Combine both\n",
    "all_docs = pdf_docs + web_docs\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} documents total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846094d",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede93da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 241\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # characters per chunk\n",
    "    chunk_overlap=100    # overlap to preserve context\n",
    ")\n",
    "\n",
    "# Split all docs\n",
    "split_docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "print(f\"Total chunks created: {len(split_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b8298",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import index, SQLRecordManager\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# 1. Set up the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "# 3. Set up Record Manager with SQLite\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace=\"faiss_index\",\n",
    "    db_url=\"sqlite:///record_manager.db\"\n",
    ")\n",
    "record_manager.create_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f60b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexing complete: {'num_added': 241, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# 4. Perform indexing using LangChain's API\n",
    "results = index(\n",
    "    split_docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",         # or \"full\", \"scoped_full\", \"none\"\n",
    "    source_id_key=\"source\"         \n",
    ")\n",
    "\n",
    "print(\"✅ Indexing complete:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc153c",
   "metadata": {},
   "source": [
    "### Adding more resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e769c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 134 documents from 4 PDFs and 3 web pages.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, WebBaseLoader\n",
    "\n",
    "# === PDF Files ===\n",
    "pdf_paths = [\n",
    "    \"PDFs/Erickson_Kretschmer_Mendis_chapter_4_PD.pdf\",\n",
    "    \"PDFs/Vox-Jenkins.pdf\",  # Public domain + culture\n",
    "    \"PDFs/Public Domain and Access to Knowledge.pdf\",  # DigitalCommons UGA\n",
    "    \"PDFs/Giblin - What Happens When Books Enter the Public Domain.pdf\"  # Harvard Ruggie\n",
    "]\n",
    "\n",
    "# Load all PDFs\n",
    "pdf_docs = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    pdf_docs.extend(loader.load())\n",
    "\n",
    "# === Web Pages ===\n",
    "web_urls = [\n",
    "    \"https://medium.com/@vikrampande783/introduction-to-langchain-9e09aae37e62\",  # Original\n",
    "    \"https://www.digitalocean.com/community/tutorials/langchain-language-model\",  # Source #6\n",
    "    \"https://www.elastic.co/blog/langchain-tutorial\"  # Source #7\n",
    "]\n",
    "\n",
    "web_docs = []\n",
    "for url in web_urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    web_docs.extend(loader.load())\n",
    "\n",
    "# === Combine All Documents ===\n",
    "all_docs = pdf_docs + web_docs\n",
    "\n",
    "print(f\"✅ Loaded {len(all_docs)} documents from {len(pdf_paths)} PDFs and {len(web_urls)} web pages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "467b48db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1129\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # characters per chunk\n",
    "    chunk_overlap=100    # overlap to preserve context\n",
    ")\n",
    "\n",
    "# Split all docs\n",
    "split_docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "print(f\"Total chunks created: {len(split_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Re-indexing complete: {'num_added': 1019, 'num_updated': 0, 'num_skipped': 108, 'num_deleted': 133}\n"
     ]
    }
   ],
   "source": [
    "# Reuse the same record manager\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace=\"faiss_index\",\n",
    "    db_url=\"sqlite:///record_manager.db\"\n",
    ")\n",
    "\n",
    "# Index new docs\n",
    "results = index(\n",
    "    split_docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\"\n",
    ")\n",
    "\n",
    "print(\"✅ Re-indexing complete:\", results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053bd708",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54808319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access groq_key\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34472e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f247201",
   "metadata": {},
   "source": [
    "### QA Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98203122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youssef_Mahmoud\\AppData\\Local\\Temp\\ipykernel_27336\\3602949890.py:16: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Answer:\n",
      " Based on the provided context, the main findings about the value of the public domain can be inferred from the works of R. Pollock, 'The Value of the Public Domain' (2006). However, the specific findings are not explicitly stated in the given text.\n",
      "\n",
      "But, R. Pollock's work is mentioned as 'The Value of the Public Domain' (London: Institute for Public Policy Research, 2006), available at http://rufuspollock.org/papers/value_of_public_domain.ippr.pdf (accessed 30 September 2018). \n",
      "\n",
      "This suggests that R. Pollock's work provides insights into the value of the public domain, but the actual findings are not provided in the given context.\n",
      "\n",
      "📎 Source Document:\n",
      " Erickson_Kretschmer_Mendis_chapter_4_PD.pdf\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Reuse vectorstore and llm from your previous cells\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # other types: \"map_reduce\", \"refine\"\n",
    "    return_source_documents=True  # optional: to see which docs were used\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What are the main findings about the value of the public domain?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Output\n",
    "print(\"💬 Answer:\\n\", result[\"result\"])\n",
    "print(\"\\n📎 Source Document:\\n\", result[\"source_documents\"][0].metadata.get(\"source\", \"No source found\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb361126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 Answer:\n",
      " LangChain is an open-source software framework that integrates Large Language Models (LLMs) into domain-specific applications. Released in October 2022, it has gained popularity in the industry and research for its easy-to-use interface. LangChain is designed to simplify the development, productionization, and deployment of LLM-powered applications.\n",
      "\n",
      "LangChain has a set of building blocks for almost every stage of the LLM application lifecycle, making it a versatile tool for various use cases. Some of the main use cases of LangChain include:\n",
      "\n",
      "1. **Chatbots and Conversational AI**: LangChain can be used to build chatbots that can understand and respond to user queries, using LLMs to generate human-like responses.\n",
      "2. **Text Summarization and Generation**: LangChain can be used to summarize long pieces of text, generate text based on a prompt, or even create entire articles.\n",
      "3. **Question Answering and Knowledge Retrieval**: LangChain can be used to build question-answering systems that can retrieve information from large datasets or the internet.\n",
      "4. **Content Generation and Creation**: LangChain can be used to generate content such as articles, social media posts, or even entire books.\n",
      "5. **Data Augmentation and Annotation**: LangChain can be used to generate synthetic data, annotate data, or even create entire datasets.\n",
      "6. **Research and Development**: LangChain can be used as a research tool to explore the capabilities of LLMs and develop new applications.\n",
      "\n",
      "Overall, LangChain provides a flexible and easy-to-use framework for building a wide range of LLM-powered applications, making it a valuable tool for developers, researchers, and businesses alike.\n",
      "\n",
      "📎 Source:\n",
      " https://medium.com/@vikrampande783/introduction-to-langchain-9e09aae37e62\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain and what are its main use cases?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(\"💬 Answer:\\n\", result[\"result\"])\n",
    "print(\"\\n📎 Source:\\n\", result[\"source_documents\"][0].metadata.get(\"source\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3707cdd",
   "metadata": {},
   "source": [
    "### Gradio & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "514d4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"document_search\",\n",
    "    description=\"Use this tool to search information from uploaded documents\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1350cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[retriever_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b738d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat_with_docs(message, history):\n",
    "    return agent.run(message)\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat_with_docs,\n",
    "    title=\"📚 Ask Your Documents (LLaMA 3)\",\n",
    "    chatbot=gr.Chatbot(show_copy_button=True),\n",
    "    examples=[\n",
    "        # 📘 PDF: Erickson_Kretschmer_Mendis_chapter_4_PD.pdf\n",
    "        \"What is the legal framework discussed in Chapter 4 on the public domain?\",\n",
    "        \"How do the authors define cultural commons?\",\n",
    "\n",
    "        # 📕 PDF: Vox-Jenkins.pdf\n",
    "        \"Why does Vox argue that the public domain is shrinking?\",\n",
    "        \"How does the public domain support creativity according to the Vox PDF?\",\n",
    "\n",
    "        # 📗 PDF: Public Domain and Access to Knowledge.pdf\n",
    "        \"What role does the public domain play in access to knowledge?\",\n",
    "        \"How does copyright affect the spread of knowledge?\",\n",
    "\n",
    "        # 📙 PDF: What Happens When Books Enter the Public Domain.pdf\n",
    "        \"What are the main effects of books entering the public domain?\",\n",
    "        \"How does the public benefit when copyright expires?\",\n",
    "\n",
    "        # 🌐 Web: Medium - Introduction to LangChain\n",
    "        \"What is LangChain and why is it useful?\",\n",
    "        \"What are document loaders in LangChain?\",\n",
    "\n",
    "        # 🌐 Web: DigitalOcean - LangChain Guide\n",
    "        \"How does LangChain integrate with LLMs?\",\n",
    "        \"What is a simple chain example from the DigitalOcean tutorial?\",\n",
    "\n",
    "        # 🌐 Web: Elastic - LangChain Tutorial\n",
    "        \"How can LangChain be used with vector databases?\",\n",
    "        \"What does Elastic suggest for building RAG apps?\"\n",
    "    ],\n",
    "    theme=\"default\",\n",
    "    type=\"messages\"\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
