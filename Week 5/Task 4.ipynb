{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55606469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access groq_key\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(groq_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bac7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from surya.recognition import RecognitionPredictor\n",
    "from surya.detection import DetectionPredictor\n",
    "from surya.layout import LayoutPredictor\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 2. Initialize predictors\n",
    "det_predictor = DetectionPredictor()\n",
    "rec_predictor = RecognitionPredictor()\n",
    "layout_predictor = LayoutPredictor()\n",
    "\n",
    "# 3. Initialize Groq LLaMA-3\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd525738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.33s/it]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.69s/it]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.23s/it]\n",
      "Recognizing layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.92s/it]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.82s/it]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.26s/it]\n",
      "Recognizing layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.16s/it]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.14s/it]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:48<00:00,  1.38s/it]\n",
      "Recognizing layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.42s/it]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.07s/it]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [01:52<00:00,  1.10s/it]\n",
      "Recognizing layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.82s/it]\n",
      "Detecting bboxes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.46s/it]\n",
      "Recognizing Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 172/172 [04:08<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# 4. Global OCR context store\n",
    "ocr_context = {\"text_output\": \"\", \"layout\": None, \"detection\": None}\n",
    "\n",
    "# 5. Process image function\n",
    "def process_image(image_pil):\n",
    "    global ocr_context\n",
    "\n",
    "    # Convert to OpenCV format\n",
    "    image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
    "    original_image = image_cv.copy()\n",
    "\n",
    "    ### Layout Predictions ---\n",
    "    layout_predictions = layout_predictor([image_pil])\n",
    "\n",
    "    layout_image = original_image.copy()\n",
    "    for box in layout_predictions[0].bboxes:\n",
    "        pts = np.array(box.polygon, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(layout_image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        label_text = f\"{box.label} ({box.confidence:.2f})\"\n",
    "        x, y = int(box.polygon[0][0]), int(box.polygon[0][1]) - 10\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "        cv2.rectangle(layout_image, (x, y - text_h - baseline), (x + text_w, y + baseline), (0, 255, 0), -1)\n",
    "        cv2.putText(layout_image, label_text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    ### Detection + Recognition Predictions ---\n",
    "    detection_image = original_image.copy()\n",
    "    predictions = rec_predictor([image_pil], det_predictor=det_predictor)\n",
    "\n",
    "    text_output = \"\"\n",
    "    for line in predictions[0].text_lines:\n",
    "        pts = np.array(line.polygon, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(detection_image, [pts], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "        reshaped_text = arabic_reshaper.reshape(line.text)\n",
    "        bidi_text = get_display(reshaped_text)\n",
    "\n",
    "        image_for_pil = Image.fromarray(cv2.cvtColor(detection_image, cv2.COLOR_BGR2RGB))\n",
    "        draw = ImageDraw.Draw(image_for_pil)\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        x, y = int(line.polygon[0][0]), int(line.polygon[0][1])\n",
    "        draw.text((x, y - 25), bidi_text, font=font, fill=(255, 0, 0))\n",
    "        detection_image = cv2.cvtColor(np.array(image_for_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        text_output += f\"{line.text}\\n\"\n",
    "\n",
    "    ### Store OCR context for QA ---\n",
    "    ocr_context[\"text_output\"] = text_output\n",
    "    ocr_context[\"layout\"] = layout_predictions\n",
    "    ocr_context[\"detection\"] = predictions\n",
    "\n",
    "    ### --- Convert images to RGB for Gradio display ---\n",
    "    layout_rgb = cv2.cvtColor(layout_image, cv2.COLOR_BGR2RGB)\n",
    "    detection_rgb = cv2.cvtColor(detection_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return layout_rgb, detection_rgb, text_output\n",
    "\n",
    "# 6. QA function using LLaMA-3\n",
    "def answer_question(question):\n",
    "    global ocr_context\n",
    "\n",
    "    template = \"\"\"\n",
    "You are a document QA assistant.\n",
    "\n",
    "Here is the extracted OCR text from the uploaded document:\n",
    "\n",
    "{ocr_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"ocr_text\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    final_prompt = prompt.format(\n",
    "        ocr_text=ocr_context.get(\"text_output\", \"\"),\n",
    "        question=question,\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content\n",
    "\n",
    "# 7. Gradio Interface\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# ðŸ“„ OCR + LLaMA-3 QA Demo\")\n",
    "    gr.Markdown(\"Upload an image, view OCR outputs, and ask questions using LLaMA-3 via Groq.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        image_input = gr.Image(type=\"pil\", label=\"Upload Document Image\")\n",
    "        layout_output = gr.Image(type=\"numpy\", label=\"Layout Predictions\")\n",
    "        detection_output = gr.Image(type=\"numpy\", label=\"Detection Predictions\")\n",
    "\n",
    "    text_output = gr.Textbox(label=\"OCR Text Output\", lines=10)\n",
    "\n",
    "    image_input.upload(process_image, inputs=image_input, outputs=[layout_output, detection_output, text_output])\n",
    "\n",
    "    gr.Markdown(\"### â“ Ask a Question about the document\")\n",
    "    question_input = gr.Textbox(label=\"Your Question\")\n",
    "    answer_output = gr.Textbox(label=\"LLaMA-3 Answer\")\n",
    "\n",
    "    question_input.submit(answer_question, inputs=question_input, outputs=answer_output)\n",
    "\n",
    "\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
